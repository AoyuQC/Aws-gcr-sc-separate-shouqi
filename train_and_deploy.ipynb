{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm libsndfile-1.0.28.tar.gz\n",
    "rm -fr libsndfile-1.0.28\n",
    "wget http://www.mega-nerd.com/libsndfile/files/libsndfile-1.0.28.tar.gz\n",
    "tar -xzvf ./libsndfile-1.0.28.tar.gz\n",
    "cd ./libsndfile-1.0.28\n",
    "./configure --prefix=/usr    \\\n",
    "            --disable-static \\\n",
    "            --docdir=/usr/share/doc/libsndfile-1.0.28 &&\n",
    "make\n",
    "sudo make install\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /home/ec2-user/.local/lib/python3.6/site-packages (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (1.16.4)\n",
      "Requirement already satisfied: numba>=0.43.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (0.48.0)\n",
      "Requirement already satisfied: six>=1.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (1.14.0)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /home/ec2-user/.local/lib/python3.6/site-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: soundfile>=0.9.0 in /home/ec2-user/.local/lib/python3.6/site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /home/ec2-user/.local/lib/python3.6/site-packages (from librosa) (2.1.8)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (0.22.1)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (0.14.1)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from numba>=0.43.0->librosa) (0.31.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from numba>=0.43.0->librosa) (46.1.3.post20200330)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from soundfile>=0.9.0->librosa) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa) (2.20)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa --user\n",
    "#!pip install sndfile\n",
    "import os\n",
    "import subprocess\n",
    "import sagemaker\n",
    "import torch\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = 'uwo-bkt-xj' # custom bucket name.\n",
    "prefix = 'audio-extraction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://uwo-bkt-xj/audio-extraction\n"
     ]
    }
   ],
   "source": [
    "s3_data_path = \"s3://{}/{}\".format(bucket, prefix)\n",
    "print(s3_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: train_sample_v6/tr/s2.json to s3://uwo-bkt-xj/audio-extraction/train/s2.json\n",
      "upload: train_sample_v6/tr/s1.json to s3://uwo-bkt-xj/audio-extraction/train/s1.json\n",
      "upload: train_sample_v6/tr/mix_both.json to s3://uwo-bkt-xj/audio-extraction/train/mix_both.json\n",
      "upload: train_sample_v6/cv/s2.json to s3://uwo-bkt-xj/audio-extraction/test/s2.json\n",
      "upload: train_sample_v6/cv/s1.json to s3://uwo-bkt-xj/audio-extraction/test/s1.json\n",
      "upload: train_sample_v6/cv/mix_both.json to s3://uwo-bkt-xj/audio-extraction/test/mix_both.json\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import json\n",
    "sample_rate=16000\n",
    "def generate_json(in_dir,out_dir,out_filename,ttype):\n",
    "    file_infos = []\n",
    "    in_dir = os.path.abspath(in_dir)\n",
    "    wav_list = os.listdir(in_dir)\n",
    "    for wav_file in wav_list:\n",
    "        if not wav_file.endswith('.wav'):\n",
    "            continue\n",
    "        wav_path = os.path.join(in_dir, wav_file)\n",
    "        samples, _ = librosa.load(wav_path, sr=sample_rate)\n",
    "        fake_wav_path = os.path.join('/opt/ml/input/data', ttype, out_filename, wav_file)\n",
    "        file_infos.append((fake_wav_path, len(samples)))\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    with open(os.path.join(out_dir, out_filename + '.json'), 'w') as f:\n",
    "        json.dump(file_infos, f, indent=4)\n",
    "        \n",
    "generate_json('train_sample_v6/tr/mix_both','train_sample_v6/tr','mix_both','train')\n",
    "generate_json('train_sample_v6/tr/s1','train_sample_v6/tr','s1','train')\n",
    "generate_json('train_sample_v6/tr/s2','train_sample_v6/tr','s2','train')\n",
    "generate_json('train_sample_v6/cv/mix_both','train_sample_v6/cv','mix_both','test')\n",
    "generate_json('train_sample_v6/cv/s1','train_sample_v6/cv','s1','test')\n",
    "generate_json('train_sample_v6/cv/s2','train_sample_v6/cv','s2','test')\n",
    "\n",
    "!aws s3 sync ./train_sample_v6/tr/ s3://uwo-bkt-xj/audio-extraction/train\n",
    "!aws s3 sync ./train_sample_v6/cv/ s3://uwo-bkt-xj/audio-extraction/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练和部署流程\n",
    "### 设置训练和验证数据路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #anbei\n",
    "# train_inputs=\"s3://sagemaker-us-east-2-310850127430/shouqi/train/\"\n",
    "# test_inputs=\"s3://sagemaker-us-east-2-310850127430/shouqi/test/\"\n",
    "#aoyu\n",
    "#train_inputs=\"s3://sagemaker-us-east-1-002224604296/separate/train/\"\n",
    "#test_inputs=\"s3://sagemaker-us-east-1-002224604296/separate/test/\"\n",
    "#xj\n",
    "train_inputs=\"s3://uwo-bkt-xj/audio-extraction/train/\"\n",
    "test_inputs=\"s3://uwo-bkt-xj/audio-extraction/test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用sagemaker pytorch进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-25 00:54:39 Starting - Starting the training job...\n",
      "2020-06-25 00:54:41 Starting - Launching requested ML instances......\n",
      "2020-06-25 00:55:51 Starting - Preparing the instances for training......\n",
      "2020-06-25 00:57:01 Downloading - Downloading input data...\n",
      "2020-06-25 00:57:30 Training - Downloading the training image........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-06-25 00:58:47,387 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-06-25 00:58:47,412 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-06-25 00:58:47,419 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-06-25 00:58:47,706 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy==1.16.4 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mCollecting numba==0.48\n",
      "  Downloading numba-0.48.0-cp36-cp36m-manylinux1_x86_64.whl (2.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning==0.7.6\n",
      "  Downloading pytorch_lightning-0.7.6-py3-none-any.whl (248 kB)\u001b[0m\n",
      "\u001b[34mCollecting Cython==0.29.20\n",
      "  Downloading Cython-0.29.20-cp36-cp36m-manylinux1_x86_64.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting asteroid==0.3.0\n",
      "  Downloading asteroid-0.3.0-py3-none-any.whl (175 kB)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker-inference==1.3.2.post0\n",
      "  Downloading sagemaker_inference-1.3.2.post0.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML==5.3.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (5.3.1)\u001b[0m\n",
      "\u001b[34mCollecting llvmlite<0.32.0,>=0.31.0dev0\n",
      "  Downloading llvmlite-0.31.0-cp36-cp36m-manylinux1_x86_64.whl (20.2 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from numba==0.48->-r requirements.txt (line 2)) (47.3.0.post20200616)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (0.17.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard>=1.14\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (1.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (4.46.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from asteroid==0.3.0->-r requirements.txt (line 5)) (1.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from asteroid==0.3.0->-r requirements.txt (line 5)) (0.25.0)\u001b[0m\n",
      "\u001b[34mCollecting torch-stoi\n",
      "  Downloading torch_stoi-0.1.1.tar.gz (5.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting soundfile\n",
      "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting pb-bss-eval\n",
      "  Downloading pb_bss_eval-0.0.2-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting torch-optimizer\n",
      "  Downloading torch_optimizer-0.0.1a13-py3-none-any.whl (35 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sagemaker-inference==1.3.2.post0->-r requirements.txt (line 6)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.6/site-packages (from sagemaker-inference==1.3.2.post0->-r requirements.txt (line 6)) (5.6.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: retrying==1.3.3 in /opt/conda/lib/python3.6/site-packages (from sagemaker-inference==1.3.2.post0->-r requirements.txt (line 6)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (0.34.2)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.30.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->asteroid==0.3.0->-r requirements.txt (line 5)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->asteroid==0.3.0->-r requirements.txt (line 5)) (2020.1)\u001b[0m\n",
      "\u001b[34mCollecting pystoi\n",
      "  Downloading pystoi-0.3.3.tar.gz (7.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting torchaudio\n",
      "  Downloading torchaudio-0.5.1-cp36-cp36m-manylinux1_x86_64.whl (3.2 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.6/site-packages (from soundfile->asteroid==0.3.0->-r requirements.txt (line 5)) (1.14.0)\u001b[0m\n",
      "\u001b[34mCollecting pesq\n",
      "  Downloading pesq-0.0.1.tar.gz (79 kB)\u001b[0m\n",
      "\u001b[34mCollecting mir-eval\n",
      "  Downloading mir_eval-0.6.tar.gz (87 kB)\u001b[0m\n",
      "\u001b[34mCollecting einops\n",
      "  Downloading einops-0.2.0-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting cached-property\n",
      "  Downloading cached_property-1.5.1-py2.py3-none-any.whl (6.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pytorch-ranger>=0.1.1\n",
      "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (1.25.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (2020.4.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (2.8)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (3.4.2)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (1.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.0->soundfile->asteroid==0.3.0->-r requirements.txt (line 5)) (2.20)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.7.6->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker-inference, torch-stoi, absl-py, pystoi, pesq, mir-eval\n",
      "  Building wheel for sagemaker-inference (setup.py): started\n",
      "  Building wheel for sagemaker-inference (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker-inference: filename=sagemaker_inference-1.3.2.post0-py2.py3-none-any.whl size=26353 sha256=973cd494ab54e2d084df0cce35b28313d1bfb5fafc2192ff056101b1863c7b57\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/60/64/fac8a3ea90f4ba71e21947e5e3a4650ec4339b94823368d988\n",
      "  Building wheel for torch-stoi (setup.py): started\u001b[0m\n",
      "\n",
      "2020-06-25 00:58:46 Training - Training image download completed. Training in progress.\u001b[34m  Building wheel for torch-stoi (setup.py): finished with status 'done'\n",
      "  Created wheel for torch-stoi: filename=torch_stoi-0.1.1-py3-none-any.whl size=6130 sha256=b59ae0762ab9642d087baf2b57440817520e562108294907612ed064a6602072\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/5e/f6/9329d0f3b92be17dcb88903d9de1a53a4701e7efb996844804\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=20b176c8dc52abbafce69c652d5f9c6038e45e89e3c2971d7f9f9c191d32ecf0\n",
      "  Stored in directory: /root/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
      "  Building wheel for pystoi (setup.py): started\n",
      "  Building wheel for pystoi (setup.py): finished with status 'done'\n",
      "  Created wheel for pystoi: filename=pystoi-0.3.3-py2.py3-none-any.whl size=7782 sha256=b9c8589e39b41f8a79ed9a6b164592df5b4b3c62fc288ce8ef17db54a0fc70b1\n",
      "  Stored in directory: /root/.cache/pip/wheels/47/80/53/bef5a8298048bc5d90f4460f3db05f6b53b1dff4cfa90349bb\n",
      "  Building wheel for pesq (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for pesq (setup.py): finished with status 'done'\n",
      "  Created wheel for pesq: filename=pesq-0.0.1-cp36-cp36m-linux_x86_64.whl size=177700 sha256=fd44268678ff19c51a14c21702b0c261d11563c732a9549dd2aab18da6ab9c4f\n",
      "  Stored in directory: /root/.cache/pip/wheels/fb/f1/bb/14e674c17a92836d017bf6c1362e85cb09ce88d975050651da\n",
      "  Building wheel for mir-eval (setup.py): started\n",
      "  Building wheel for mir-eval (setup.py): finished with status 'done'\n",
      "  Created wheel for mir-eval: filename=mir_eval-0.6-py3-none-any.whl size=96513 sha256=f8ae79db15a8dc42557800249fa0e4b0f3ebeafb43ad924d3d411fcbea3176f3\n",
      "  Stored in directory: /root/.cache/pip/wheels/33/84/cf/b9da5f4e21a4d2ae3112774eb27ffd40fbd63cb68fc3195bf5\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker-inference torch-stoi absl-py pystoi pesq mir-eval\u001b[0m\n",
      "\u001b[34mInstalling collected packages: llvmlite, numba, cachetools, pyasn1-modules, google-auth, tensorboard-plugin-wit, absl-py, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, markdown, tensorboard, pytorch-lightning, Cython, pystoi, torchaudio, torch-stoi, soundfile, pesq, mir-eval, einops, cached-property, pb-bss-eval, pytorch-ranger, torch-optimizer, asteroid, sagemaker-inference\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.12\n",
      "    Uninstalling Cython-0.29.12:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled Cython-0.29.12\u001b[0m\n",
      "\u001b[34mSuccessfully installed Cython-0.29.20 absl-py-0.9.0 asteroid-0.3.0 cached-property-1.5.1 cachetools-4.1.0 einops-0.2.0 google-auth-1.18.0 google-auth-oauthlib-0.4.1 grpcio-1.30.0 llvmlite-0.31.0 markdown-3.2.2 mir-eval-0.6 numba-0.48.0 oauthlib-3.1.0 pb-bss-eval-0.0.2 pesq-0.0.1 pyasn1-modules-0.2.8 pystoi-0.3.3 pytorch-lightning-0.7.6 pytorch-ranger-0.1.1 requests-oauthlib-1.3.0 sagemaker-inference-1.3.2.post0 soundfile-0.10.3.post1 tensorboard-2.2.2 tensorboard-plugin-wit-1.6.0.post3 torch-optimizer-0.0.1a13 torch-stoi-0.1.1 torchaudio-0.5.1\u001b[0m\n",
      "\u001b[34m2020-06-25 00:59:09,908 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-06-25-00-54-39-133\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-948874832909/pytorch-training-2020-06-25-00-54-39-133/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"dprnn-train-deploy\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"dprnn-train-deploy.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=dprnn-train-deploy.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=dprnn-train-deploy\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-948874832909/pytorch-training-2020-06-25-00-54-39-133/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-06-25-00-54-39-133\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-948874832909/pytorch-training-2020-06-25-00-54-39-133/source/sourcedir.tar.gz\",\"module_name\":\"dprnn-train-deploy\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"dprnn-train-deploy.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python dprnn-train-deploy.py --epochs 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m{'positional arguments': {}, 'filterbank': {'n_filters': 64, 'kernel_size': 2, 'stride': 1}, 'masknet': {'in_chan': 64, 'n_src': 2, 'out_chan': 64, 'bn_chan': 128, 'hid_size': 128, 'chunk_size': 250, 'hop_size': 125, 'n_repeats': 6, 'mask_act': 'sigmoid', 'bidirectional': True, 'dropout': 0}, 'training': {'epochs': 1, 'batch_size': 1, 'num_workers': 1, 'half_lr': True, 'early_stop': True, 'gradient_clipping': 5}, 'optim': {'optimizer': 'adam', 'lr': 0.001, 'weight_decay': 1e-05}, 'data': {'train_dir': 'data/wav16k/min/tr/', 'valid_dir': 'data/wav16k/min/cv/', 'task': 'sep_noisy', 'nondefault_nsrc': None, 'sample_rate': 16000, 'mode': 'min', 'segment': 1.0}, 'main_args': {'help': None}}\u001b[0m\n",
      "\u001b[34mDrop 0 utts(0.00 h) from 14 (shorter than 16000 samples)\u001b[0m\n",
      "\u001b[34mDrop 0 utts(0.00 h) from 2 (shorter than 16000 samples)\u001b[0m\n",
      "\u001b[34m!!!!!!!!!\u001b[0m\n",
      "\u001b[34m(tensor([-0.0331, -0.0357, -0.0378,  ..., -0.0222, -0.0257, -0.0284]), tensor([[ 0.0367,  0.0368,  0.0362,  ...,  0.0040,  0.0011, -0.0013],\n",
      "        [-0.0331, -0.0357, -0.0378,  ..., -0.0222, -0.0257, -0.0284]]))\u001b[0m\n",
      "\u001b[34m(tensor([ 0.2913,  0.4111,  0.4866,  ...,  0.0118, -0.0179, -0.0426]), tensor([[ 0.0128,  0.0119,  0.0113,  ..., -0.0062, -0.0689, -0.1341],\n",
      "        [ 0.2913,  0.4111,  0.4866,  ...,  0.0118, -0.0179, -0.0426]]))\u001b[0m\n",
      "\u001b[34m!!!!!!!!!\u001b[0m\n",
      "\u001b[34mNCCL version 2.4.8+cuda10.1\u001b[0m\n",
      "\u001b[34m#015Validation sanity check: 0it [00:00, ?it/s][2020-06-25 00:59:22.755 algo-1:118 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-06-25 00:59:22.755 algo-1:118 INFO hook.py:191] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-06-25 00:59:22.755 algo-1:118 INFO hook.py:236] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-06-25 00:59:22.755 algo-1:118 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2020-06-25 00:59:22.758 algo-1:118 INFO hook.py:376] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-06-25 00:59:22.758 algo-1:118 INFO hook.py:437] Hook is writing from the hook with pid: 118\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-06-25 00:59:23.618 algo-1:118 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:LightningDistributedDataParallel int\u001b[0m\n",
      "\u001b[34m[2020-06-25 00:59:23.618 algo-1:118 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:LightningDistributedDataParallel dict\u001b[0m\n",
      "\n",
      "2020-06-25 00:59:38 Uploading - Uploading generated training model\u001b[34m#015Validation sanity check:  50%|█████     | 1/2 [00:02<00:02,  2.67s/it]#015                                                                      #015#015Training: 0it [00:00, ?it/s]#015Training:   0%|          | 0/16 [00:00<?, ?it/s]#015Epoch 1:   0%|          | 0/16 [00:00<?, ?it/s] #015Epoch 1:   6%|▋         | 1/16 [00:02<00:31,  2.08s/it]#015Epoch 1:   6%|▋         | 1/16 [00:02<00:31,  2.08s/it, loss=12.524, v_num=0]#015Epoch 1:  12%|█▎        | 2/16 [00:02<00:17,  1.23s/it, loss=12.524, v_num=0]#015Epoch 1:  12%|█▎        | 2/16 [00:02<00:17,  1.23s/it, loss=11.883, v_num=0]#015Epoch 1:  19%|█▉        | 3/16 [00:02<00:12,  1.06it/s, loss=11.883, v_num=0]#015Epoch 1:  19%|█▉        | 3/16 [00:02<00:12,  1.06it/s, loss=11.766, v_num=0]#015Epoch 1:  25%|██▌       | 4/16 [00:03<00:09,  1.25it/s, loss=11.766, v_num=0]#015Epoch 1:  25%|██▌       | 4/16 [00:03<00:09,  1.25it/s, loss=12.970, v_num=0]#015Epoch 1:  31%|███▏      | 5/16 [00:03<00:07,  1.39it/s, loss=12.970, v_num=0]#015Epoch 1:  31%|███▏      | 5/16 [00:03<00:07,  1.39it/s, loss=11.628, v_num=0]#015Epoch 1:  38%|███▊      | 6/16 [00:03<00:06,  1.52it/s, loss=11.628, v_num=0]#015Epoch 1:  38%|███▊      | 6/16 [00:03<00:06,  1.52it/s, loss=14.010, v_num=0]#015Epoch 1:  44%|████▍     | 7/16 [00:04<00:05,  1.63it/s, loss=14.010, v_num=0]#015Epoch 1:  44%|████▍     | 7/16 [00:04<00:05,  1.63it/s, loss=13.663, v_num=0]#015Epoch 1:  50%|█████     | 8/16 [00:04<00:04,  1.71it/s, loss=13.663, v_num=0]#015Epoch 1:  50%|█████     | 8/16 [00:04<00:04,  1.71it/s, loss=12.977, v_num=0]#015Epoch 1:  56%|█████▋    | 9/16 [00:05<00:03,  1.78it/s, loss=12.977, v_num=0]#015Epoch 1:  56%|█████▋    | 9/16 [00:05<00:03,  1.78it/s, loss=13.662, v_num=0]#015Epoch 1:  62%|██████▎   | 10/16 [00:05<00:03,  1.85it/s, loss=13.662, v_num=0]#015Epoch 1:  62%|██████▎   | 10/16 [00:05<00:03,  1.85it/s, loss=13.029, v_num=0]#015Epoch 1:  69%|██████▉   | 11/16 [00:05<00:02,  1.92it/s, loss=13.029, v_num=0]#015Epoch 1:  69%|██████▉   | 11/16 [00:05<00:02,  1.92it/s, loss=12.582, v_num=0]#015Epoch 1:  75%|███████▌  | 12/16 [00:06<00:02,  1.98it/s, loss=12.582, v_num=0]#015Epoch 1:  75%|███████▌  | 12/16 [00:06<00:02,  1.98it/s, loss=11.878, v_num=0]#015Epoch 1:  81%|████████▏ | 13/16 [00:06<00:01,  2.01it/s, loss=11.878, v_num=0]#015Epoch 1:  81%|████████▏ | 13/16 [00:06<00:01,  2.01it/s, loss=11.493, v_num=0]#015Epoch 1:  88%|████████▊ | 14/16 [00:06<00:00,  2.00it/s, loss=11.493, v_num=0]#015Epoch 1:  88%|████████▊ | 14/16 [00:06<00:00,  2.00it/s, loss=10.995, v_num=0]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  50%|█████     | 1/2 [00:01<00:01,  1.79s/it]#033[A#015Epoch 1:  94%|█████████▍| 15/16 [00:08<00:00,  1.71it/s, loss=10.995, v_num=0]#015Epoch 1: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s, loss=10.995, v_num=0, val_loss=6.07]\u001b[0m\n",
      "\u001b[34m#015                                                         #033[A#015Epoch 1: 100%|██████████| 16/16 [00:09<00:00,  1.75it/s, loss=10.995, v_num=0, val_loss=6.07]\u001b[0m\n",
      "\u001b[34m[2020-06-25 00:59:33.251 algo-1:118 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-06-25 00:59:35,580 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-06-25 00:59:55 Completed - Training job completed\n",
      "Training seconds: 174\n",
      "Billable seconds: 174\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "dprnn_estimator = PyTorch(entry_point='dprnn-train-deploy.py',\n",
    "                            source_dir='code',\n",
    "                            train_instance_type='ml.p3.2xlarge',\n",
    "                            train_instance_count=1,\n",
    "                            role=role,\n",
    "                            framework_version='1.5.1',\n",
    "                            hyperparameters = {'epochs': 1})\n",
    "dprnn_estimator.fit({'train': train_inputs,\n",
    "                       'test': test_inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将训练好的模型进行部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "# Deploy my estimator to a SageMaker Endpoint and get a Predictor\n",
    "predictor = dprnn_estimator.deploy(instance_type='ml.g4dn.xlarge',\n",
    "                                     initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在部署好的模型上进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.utils import *\n",
    "\n",
    "# 10 seconds for each \n",
    "instance_time = 10\n",
    "filename = './test_sample/audio_raw_1-20_clip_0.wav'\n",
    "# filename = './data/tt/mix/1-20-t-1_1-20-t-1_0dB.wav'\n",
    "\n",
    "x, fs = preprocess_wav(filename)\n",
    "instance_seg = instance_time * fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:47",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5bbf3ffcb968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgl_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_x\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mflat_estimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseparate_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ms_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_estimate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_estimate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/mlxj/Aws-gcr-sc-separate-shouqi/helper/utils.py\u001b[0m in \u001b[0;36mseparate_process\u001b[0;34m(x, predictor)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0milens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0milens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mmixture_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixtures_pad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mmix_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0milens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixture_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m             raise AssertionError(\n\u001b[1;32m    152\u001b[0m                 \"libcudart functions unavailable. It looks like you have a broken build?\")\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:47"
     ]
    }
   ],
   "source": [
    "l_x = len(x)\n",
    "s_x = instance_seg\n",
    "s1 = np.array([])\n",
    "s2 = np.array([])\n",
    "gl_i = 0\n",
    "for start in range(0, l_x-s_x, s_x):\n",
    "    flat_estimate = separate_process(x[start:start+s_x], predictor)\n",
    "    s1 = np.concatenate([s1, flat_estimate[0]])\n",
    "    s2 = np.concatenate([s2, flat_estimate[1]])\n",
    "    gl_i = start+s_x\n",
    "if gl_i < l_x:\n",
    "    flat_estimate = separate_process(x[gl_i:], predictor)\n",
    "    s1 = np.concatenate([s1, flat_estimate[0]])\n",
    "    s2 = np.concatenate([s2, flat_estimate[1]])\n",
    "    \n",
    "write(s1, './result/complete_split_s1.wav')\n",
    "write(s2, './result/complete_split_s2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which: no pytorch in (/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/libexec/gcc/x86_64-amazon-linux/4.8.5:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/home/ec2-user/anaconda3/envs/pytorch_p36/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/libexec/gcc/x86_64-amazon-linux/4.8.5:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/home/ec2-user/.dl_binaries/bin:/usr/local/mpi/bin:/opt/aws/neuron/bin:/home/ec2-user/anaconda3/bin:/home/ec2-user/anaconda3/condabin:/home/ec2-user/anaconda3/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/libexec/gcc/x86_64-amazon-linux/4.8.5:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/opt/aws/bin)\n"
     ]
    }
   ],
   "source": [
    "!which pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "from scipy.signal import lfilter\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "import soundfile as sf\n",
    "\n",
    "import fnmatch, os, warnings\n",
    "\n",
    "def remove_pad(inputs, inputs_lengths):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs: torch.Tensor, [B, C, T] or [B, T], B is batch size\n",
    "        inputs_lengths: torch.Tensor, [B]\n",
    "    Returns:\n",
    "        results: a list containing B items, each item is [C, T], T varies\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    dim = inputs.dim()\n",
    "    if dim == 3:\n",
    "        C = inputs.size(1)\n",
    "    for input, length in zip(inputs, inputs_lengths):\n",
    "        if dim == 3: # [B, C, T]\n",
    "            results.append(input[:,:length].view(C, -1).cpu().numpy())\n",
    "        elif dim == 2:  # [B, T]\n",
    "            results.append(input[:length].view(-1).cpu().numpy())\n",
    "    return results\n",
    "\n",
    "def write(inputs, filename, sr=16000):\n",
    "    librosa.output.write_wav(filename, inputs, sr)\n",
    "\n",
    "def wavread(filename):\n",
    "    fs, x = scipy.io.wavfile.read(filename)\n",
    "    if np.issubdtype(x.dtype, np.integer):\n",
    "        x = x / np.iinfo(x.dtype).max\n",
    "    return x, fs\n",
    "\n",
    "def wavwrite(filename, s, fs):\n",
    "    if s.dtype != np.int16:\n",
    "        s = np.array(s * 2**15, dtype=np.int16)\n",
    "    if np.any(s > np.iinfo(np.int16).max) or np.any(s < np.iinfo(np.int16).min):\n",
    "        warnings.warn('Warning: clipping detected when writing {}'.format(filename))\n",
    "    scipy.io.wavfile.write(filename, fs, s)\n",
    "\n",
    "def asl_meter(x, fs, nbits=16):\n",
    "    '''Measure the Active Speech Level (ASR) of x following ITU-T P.56.\n",
    "    If x is integer, it will be scaled to (-1, 1) according to nbits.\n",
    "    '''\n",
    "\n",
    "    if np.issubdtype(x.dtype, np.integer):\n",
    "        x = x / 2**(nbits-1)\n",
    "\n",
    "    # Constants\n",
    "    MIN_LOG_OFFSET = 1e-20\n",
    "    T = 0.03                # Time constant of smoothing in seconds\n",
    "    g = np.exp(-1/(T*fs))\n",
    "    H = 0.20                # Time of handover in seconds\n",
    "    I = int(np.ceil(H*fs))\n",
    "    M = 15.9                # Margin between threshold and ASL in dB\n",
    "\n",
    "    a = np.zeros(nbits-1)                       # Activity count\n",
    "    c = 0.5**np.arange(nbits-1, 0, step=-1)     # Threshold level\n",
    "    h = np.ones(nbits)*I                        # Hangover count\n",
    "    s = 0\n",
    "    sq = 0\n",
    "    p = 0\n",
    "    q = 0\n",
    "    asl = -100\n",
    "\n",
    "    L = len(x)\n",
    "    s = sum(abs(x))\n",
    "    sq = sum(x**2)\n",
    "    dclevel = s/np.arange(1, L+1)\n",
    "    lond_term_level = 10*np.log10(sq/np.arange(1, L+1) + MIN_LOG_OFFSET)\n",
    "    c_dB = 20*np.log10(c)\n",
    "\n",
    "    for i in range(L):\n",
    "        p = g * p + (1-g) * abs(x[i])\n",
    "        q = g * q + (1-g) * p\n",
    "\n",
    "        for j in range(nbits-1):\n",
    "            if q >= c[j]:\n",
    "                a[j] += 1\n",
    "                h[j] = 0\n",
    "            elif h[j] < I:\n",
    "                a[j] += 1\n",
    "                h[j] += 1\n",
    "\n",
    "    a_dB = -100 * np.ones(nbits-1)\n",
    "\n",
    "    for i in range(nbits-1):\n",
    "        if a[i] != 0:\n",
    "            a_dB[i] = 10*np.log10(sq/a[i])\n",
    "\n",
    "    delta = a_dB - c_dB\n",
    "    idx = np.where(delta <= M)[0]\n",
    "\n",
    "    if len(idx) != 0:\n",
    "        idx = idx[0]\n",
    "        if idx > 1:\n",
    "            asl = bin_interp(a_dB[idx], a_dB[idx-1], c_dB[idx], c_dB[idx-1], M)\n",
    "        else:\n",
    "            asl = a_dB[idx]\n",
    "\n",
    "    return asl\n",
    "\n",
    "def bin_interp(upcount, lwcount, upthr, lwthr, margin, tol=0.1):\n",
    "    n_iter = 1\n",
    "    if abs(upcount - upthr - margin) < tol:\n",
    "        midcount = upcount\n",
    "    elif abs(lwcount - lwthr - margin) < tol:\n",
    "        midcount = lwcount\n",
    "    else:\n",
    "        midcount = (upcount + lwcount)/2\n",
    "        midthr = (upthr + lwthr)/2\n",
    "        diff = midcount - midthr - margin\n",
    "        while abs(diff) > tol:\n",
    "            n_iter += 1\n",
    "            if n_iter > 20:\n",
    "                tol *= 1.1\n",
    "            if diff > tol:\n",
    "                midcount = (upcount + midcount)/2\n",
    "                midthr = (upthr + midthr)/2\n",
    "            elif diff < -tol:\n",
    "                midcount = (lwcount + midcount)/2\n",
    "                midthr = (lwthr + midthr)/2\n",
    "            diff = midcount - midthr - margin\n",
    "    return midcount\n",
    "\n",
    "\n",
    "def rms_energy(x):\n",
    "    return 10*np.log10((1e-12 + x.dot(x))/len(x))\n",
    "\n",
    "def preprocess_wav(filename):\n",
    "    # filename = './1-20.wav'\n",
    "    x, fs = wavread(filename)\n",
    "    x_mirror = x\n",
    "    # x = np.float32(x)\n",
    "    # print(x.dtype)\n",
    "    # N_dB = rms_energy(x_mirror)\n",
    "    # S_dB = asl_meter(x, fs)\n",
    "    # print(N_dB)\n",
    "    # print(S_dB)\n",
    "\n",
    "    # N_new = S_dB\n",
    "    # x_mirror = 10**(N_new/20) * x_mirror / 10**(N_dB/20)\n",
    "    asl_level = -26.0\n",
    "\n",
    "    y = x + x_mirror\n",
    "    y = y/10**(asl_meter(y, fs)/20) * 10**(asl_level/20)\n",
    "    return x, fs\n",
    "\n",
    "def pad_list(xs, pad_value):\n",
    "    n_batch = len(xs)\n",
    "    max_len = max(x.size(0) for x in xs)\n",
    "    pad = xs[0].new(n_batch, max_len, * xs[0].size()[1:]).fill_(pad_value)\n",
    "    for i in range(n_batch):\n",
    "        pad[i, :xs[i].size(0)] = xs[i]\n",
    "    return pad\n",
    "\n",
    "def separate_process(x, predictor):\n",
    "    ilens = np.array([x.shape[0]])\n",
    "\n",
    "    # perform padding and convert to tensor\n",
    "    pad_value = 0\n",
    "    mixtures_pad = pad_list([torch.from_numpy(x).float()], pad_value)\n",
    "    ilens = torch.from_numpy(ilens) \n",
    "    mixture_np = mixtures_pad.numpy()\n",
    "    mix_lengths = ilens.cuda()\n",
    "    # Forward\n",
    "    response = predictor.predict(mixture_np)\n",
    "#     print(response)\n",
    "#     estimate_source = response.argmax(axis=1)[0]\n",
    "    response = torch.from_numpy(response) \n",
    "    estimate_source = response.cuda()\n",
    "    # Remove padding and flat\n",
    "#     print(estimate_source)\n",
    "    flat_estimate = remove_pad(estimate_source, mix_lengths)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         mixture, mix_lengths = mixtures_pad.cuda(), ilens.cuda()\n",
    "\n",
    "#         # Forward\n",
    "#         estimate_source = model(mixture)  # [B, C, T]\n",
    "        \n",
    "#         # Remove padding and flat\n",
    "#         flat_estimate = remove_pad(estimate_source, mix_lengths)\n",
    "    return flat_estimate[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
